{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b84f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict,Counter\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35d07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class LabeledTextData:\n",
    "    id_num: int\n",
    "    label: str\n",
    "    tokens: list\n",
    "\n",
    "train = [LabeledTextData(42, 'cat',  \"🐈 🐯 🐱 🐩 🐱\".split()),\n",
    "         LabeledTextData(43, 'dog',  \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\".split()),\n",
    "         LabeledTextData(45, 'cat',  \"🐈 🐈 🐯 🐶 🐈\".split()),\n",
    "         LabeledTextData(45, 'cat',  \"🐈 🐈 🐈\".split()),\n",
    "         LabeledTextData(48, 'dog',  \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \".split()),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0bcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doc_priors(labels):\n",
    "    doc_priors = defaultdict(float)\n",
    "\n",
    "    for label in labels:\n",
    "        doc_priors[label] = sum(1 for d in labels if d == label) / len(labels)\n",
    "\n",
    "    return doc_priors# Number of males\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3334d84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'cat': 0.6, 'dog': 0.4})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [d.label for d in train]\n",
    "prior = doc_priors(labels)\n",
    "prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632da096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(train, labels):\n",
    "    \n",
    "    # A default dict of default dicts; inner default dict is probability\n",
    "    cond_prob = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    for label in set(labels):\n",
    "    \n",
    "        label_tokens = []\n",
    "        for i, doc in enumerate(train):\n",
    "             # For a given label, get a list of all the tokens for all the docs \n",
    "            if labels[i] == label:\n",
    "                label_tokens.extend(doc)\n",
    "\n",
    "        for token in set(label_tokens):\n",
    "            # Find conditional probability: token count / total count\n",
    "            cond_prob[label][token] = label_tokens.count(token) / len(label_tokens) \n",
    "\n",
    "    return cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3261b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.likelihood.<locals>.<lambda>()>,\n",
       "            {'cat': defaultdict(float,\n",
       "                         {'🐩': 0.07692307692307693,\n",
       "                          '🐈': 0.5384615384615384,\n",
       "                          '🐱': 0.15384615384615385,\n",
       "                          '🐶': 0.07692307692307693,\n",
       "                          '🐯': 0.15384615384615385}),\n",
       "             'dog': defaultdict(float,\n",
       "                         {'🐩': 0.2222222222222222,\n",
       "                          '🐈': 0.16666666666666666,\n",
       "                          '🐱': 0.05555555555555555,\n",
       "                          '🐶': 0.5,\n",
       "                          '🐯': 0.05555555555555555})})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n",
    "train_x =[x.tokens for x in train]\n",
    "cond_prob = likelihood(train_x, labels)\n",
    "cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fda316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cat', 'dog'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_prob.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4f54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "def post_prob(test, cond_prob, doc_priors):\n",
    "    prob_predicted = defaultdict(float)\n",
    "    for label in cond_prob.keys():\n",
    "        # For each label, calculate the conditional probability based on the prior and the tokens that appear\n",
    "        prob_predicted[label] = doc_priors[label] * product(cond_prob[label][t] for t in test)\n",
    "    \n",
    "    return prob_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8045c96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'cat': 0.09230769230769231, 'dog': 0.022222222222222223})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = LabeledTextData(id_num=90, label=None, tokens=\"🐱\".split())\n",
    "# test = LabeledTextData(id_num=91, label=None, tokens=\"🐶 🐶\".split()) \n",
    "# test = LabeledTextData(id_num=92, label=None, tokens=\"🐶 🐱\".split())\n",
    "# test = LabeledTextData(id_num=93, label=None, tokens=\"🐈 🐈 🐶 🐶 🐩 🐱 🐱\".split())\n",
    "# test = LabeledTextData(id_num=94, label=None, tokens=\"🐬 \".split()) # Out of sample prediction\n",
    "test_x = test.tokens\n",
    "prob_predicted = post_prob(test_x, cond_prob, prior)\n",
    "prob_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04614565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: cat\n"
     ]
    }
   ],
   "source": [
    "label, prob = max(prob_predicted.items(),\n",
    "                  key=itemgetter(1))\n",
    "print(\"The predicted class is:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2871d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes_MN:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.labelset = None\n",
    "        self.n_items = None\n",
    "        self.priors = defaultdict(float)\n",
    "        self.prob_predicted = defaultdict(float)\n",
    "        self.c_word_dict = defaultdict(lambda: defaultdict(float))\n",
    "        self.class_count = defaultdict(float)\n",
    "    \n",
    "    def fit(self, x_train, labels):\n",
    "        \"\"\"Train the Naïve Bayes classifier using text data.\"\"\"\n",
    "        if not isinstance(x_train, list) or not all(isinstance(doc, list) for doc in x_train):\n",
    "            raise ValueError(\"x_train must be a list of tokenized documents (list of lists).\")\n",
    "        \n",
    "        self.labels = np.array(labels)\n",
    "        self.n_items = len(labels)\n",
    "        self.labelset = set(labels)\n",
    "\n",
    "        word_dict = defaultdict(list)\n",
    "        \n",
    "        # Collect words for each label and build vocabulary\n",
    "        for i, doc in enumerate(x_train):\n",
    "            self.vocab.update(doc)  # Using a set to avoid duplicates\n",
    "            word_dict[labels[i]].extend(doc)\n",
    "\n",
    "        # Convert vocab to list for indexing\n",
    "        self.vocab = list(self.vocab)\n",
    "        self.vocab_len = len(self.vocab)\n",
    "\n",
    "        # Count occurrences of words in each class\n",
    "        for label in self.labelset:\n",
    "            self.c_word_dict[label] = Counter(word_dict[label])\n",
    "            self.class_count[label] = sum(self.c_word_dict[label].values())\n",
    "\n",
    "        # Compute prior probabilities\n",
    "        self.doc_priors()\n",
    "\n",
    "    def doc_priors(self):\n",
    "        \"\"\"Calculate prior probabilities of each class.\"\"\"\n",
    "        for label in self.labelset:\n",
    "            self.priors[label] = np.sum(self.labels == label) / self.n_items\n",
    "\n",
    "    def likelihood(self, word, label):\n",
    "        \"\"\"Calculate log-likelihood of a word given a class label.\"\"\"\n",
    "        return np.log((self.c_word_dict[label][word] + 1.0) / \n",
    "                      (self.class_count[label] + self.vocab_len + 1.0))\n",
    "\n",
    "    def post_prob(self, test):\n",
    "        \"\"\"Calculate posterior probabilities for a test document.\"\"\"\n",
    "        for label in self.labelset:\n",
    "            self.prob_predicted[label] = np.log(self.priors[label]) \n",
    "            for word in test:\n",
    "                self.prob_predicted[label] += self.likelihood(word, label)\n",
    "        return self.prob_predicted\n",
    "\n",
    "    def predict(self, test):\n",
    "        \"\"\"Predict labels for a given test dataset.\"\"\"\n",
    "        self.test_labels = []\n",
    "        for doc in test:\n",
    "            prob_predicted = self.post_prob(doc)\n",
    "            label, _ = max(prob_predicted.items(), key=itemgetter(1))\n",
    "            self.test_labels.append(label)\n",
    "        return self.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1169f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayes_MN()\n",
    "clf.fit(train_x, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c1e61db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'cat': 13, 'dog': 18})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f7a92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🐱']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceb49b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf.predict([test_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7109ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories=['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med'] \n",
    "newsgroups_train=fetch_20newsgroups(subset='train',categories=categories)\n",
    "\n",
    "train_data=newsgroups_train.data #getting all trainign examples\n",
    "train_labels=newsgroups_train.target #getting training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):\n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) #every char except alphabets is replaced\n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n",
    "    cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case\n",
    "    \n",
    "    return cleaned_str # returning the preprocessed string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ff0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = [preprocess_string(x).split() for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4729250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayes_MN()\n",
    "clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ee04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test=fetch_20newsgroups(subset='test',categories=categories) #loading test data\n",
    "test_data=newsgroups_test.data #get test set examples\n",
    "test_labels=newsgroups_test.target #get test set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [preprocess_string(x).split() for x in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted accuracy is %.3f\" %(np.sum(test_labels == predicted)/len(predicted)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f41422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da18504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
